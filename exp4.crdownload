#!/usr/bin/env python
# coding: utf-8

# In[2]:


#exp no:5
import numpy as np  

class Word2Vec:  
    def __init__(self, corpus, embedding_dim, window_size=2, learning_rate=0.01):  
        self.corpus = corpus  
        self.embedding_dim = embedding_dim  
        self.window_size = window_size  
        self.learning_rate = learning_rate  
        self.word2id = {}  
        self.id2word = {}  
        self.vocab_size = 0  
        self.training_data = []  
        self.initialize()  

    def initialize(self):  
        words = [word for sentence in self.corpus for word in sentence]  
        unique_words = set(words)  
        self.vocab_size = len(unique_words)  
        for i, word in enumerate(unique_words):  
            self.word2id[word] = i  
            self.id2word[i] = word  

    def generate_training_data(self):  
        for sentence in self.corpus:  
            for i, target_word in enumerate(sentence):  
                context_words = []  
                for j in range(i - self.window_size, i + self.window_size + 1):  
                    if j != i and j >= 0 and j < len(sentence):  
                        context_words.append(sentence[j])  
                if context_words:  
                    self.training_data.append((context_words, target_word))  

    def initialize_weights(self):  
        self.input_weights = np.random.uniform(-1, 1, (self.vocab_size, self.embedding_dim))  
        self.output_weights = np.random.uniform(-1, 1, (self.embedding_dim, self.vocab_size))  

    def softmax(self, x):  
        exp_scores = np.exp(x - np.max(x))  
        return exp_scores / np.sum(exp_scores, axis=0)  

    def forward_propagation(self, context_words):  
        context_ids = [self.word2id[word] for word in context_words]  
        context_vectors = self.input_weights[context_ids]  
        hidden_vector = np.mean(context_vectors, axis=0)  
        output_vector = np.dot(hidden_vector, self.output_weights)  
        output_probs = self.softmax(output_vector)  
        return context_ids, hidden_vector, output_probs  

    def backward_propagation(self, context_ids, hidden_vector, output_probs, target_word):  
        target_id = self.word2id[target_word]  
        output_probs[target_id] -= 1  
        delta_output_weights = np.outer(hidden_vector, output_probs)  
        delta_hidden = np.dot(self.output_weights, output_probs.T)  
        self.output_weights -= self.learning_rate * delta_output_weights  
        for word_id in context_ids:  
            self.input_weights[word_id] -= self.learning_rate * delta_hidden / len(context_ids)  

    def train(self, epochs):  
        self.initialize_weights()  
        self.generate_training_data()  
        for epoch in range(epochs):  
            loss = 0  
            for context_words, target_word in self.training_data:  
                context_ids, hidden_vector, output_probs = self.forward_propagation(context_words)  
                self.backward_propagation(context_ids, hidden_vector, output_probs, target_word)  
                loss += -np.log(output_probs[self.word2id[target_word]] + 1e-9)  
            if (epoch + 1) % 10 == 0:  
                print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss / len(self.training_data)}")  

    def get_word_vector(self, word):  
        return self.input_weights[self.word2id[word]]  


corpus = [["I", "love", "machine", "learning"], ["Word2Vec", "is", "awesome"]]  
model = Word2Vec(corpus, embedding_dim=50, window_size=1, learning_rate=0.01)  
model.train(epochs=50)  
print(model.get_word_vector("machine"))  


# In[ ]:




