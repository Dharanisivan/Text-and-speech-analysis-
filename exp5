#exp no:5
import numpy as np  

class Word2Vec:  
    def __init__(self, corpus, embedding_dim, window_size=2, learning_rate=0.01):  
        self.corpus = corpus  
        self.embedding_dim = embedding_dim  
        self.window_size = window_size  
        self.learning_rate = learning_rate  
        self.word2id = {}  
        self.id2word = {}  
        self.vocab_size = 0  
        self.training_data = []  
        self.initialize()  

    def initialize(self):  
        words = [word for sentence in self.corpus for word in sentence]  
        unique_words = set(words)  
        self.vocab_size = len(unique_words)  
        for i, word in enumerate(unique_words):  
            self.word2id[word] = i  
            self.id2word[i] = word  

    def generate_training_data(self):  
        for sentence in self.corpus:  
            for i, target_word in enumerate(sentence):  
                context_words = []  
                for j in range(i - self.window_size, i + self.window_size + 1):  
                    if j != i and j >= 0 and j < len(sentence):  
                        context_words.append(sentence[j])  
                if context_words:  
                    self.training_data.append((context_words, target_word))  

    def initialize_weights(self):  
        self.input_weights = np.random.uniform(-1, 1, (self.vocab_size, self.embedding_dim))  
        self.output_weights = np.random.uniform(-1, 1, (self.embedding_dim, self.vocab_size))  

    def softmax(self, x):  
        exp_scores = np.exp(x - np.max(x))  
        return exp_scores / np.sum(exp_scores, axis=0)  

    def forward_propagation(self, context_words):  
        context_ids = [self.word2id[word] for word in context_words]  
        context_vectors = self.input_weights[context_ids]  
        hidden_vector = np.mean(context_vectors, axis=0)  
        output_vector = np.dot(hidden_vector, self.output_weights)  
        output_probs = self.softmax(output_vector)  
        return context_ids, hidden_vector, output_probs  

    def backward_propagation(self, context_ids, hidden_vector, output_probs, target_word):  
        target_id = self.word2id[target_word]  
        output_probs[target_id] -= 1  
        delta_output_weights = np.outer(hidden_vector, output_probs)  
        delta_hidden = np.dot(self.output_weights, output_probs.T)  
        self.output_weights -= self.learning_rate * delta_output_weights  
        for word_id in context_ids:  
            self.input_weights[word_id] -= self.learning_rate * delta_hidden / len(context_ids)  

    def train(self, epochs):  
        self.initialize_weights()  
        self.generate_training_data()  
        for epoch in range(epochs):  
            loss = 0  
            for context_words, target_word in self.training_data:  
                context_ids, hidden_vector, output_probs = self.forward_propagation(context_words)  
                self.backward_propagation(context_ids, hidden_vector, output_probs, target_word)  
                loss += -np.log(output_probs[self.word2id[target_word]] + 1e-9)  
            if (epoch + 1) % 10 == 0:  
                print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss / len(self.training_data)}")  

    def get_word_vector(self, word):  
        return self.input_weights[self.word2id[word]]  


corpus = [["I", "love", "machine", "learning"], ["Word2Vec", "is", "awesome"]]  
model = Word2Vec(corpus, embedding_dim=50, window_size=1, learning_rate=0.01)  
model.train(epochs=50)  
print(model.get_word_vector("machine"))  
output:
Epoch 10/50, Loss: nan
Epoch 20/50, Loss: nan
Epoch 30/50, Loss: nan
Epoch 40/50, Loss: nan
Epoch 50/50, Loss: nan
[-0.67427039 -0.28956927 -0.75475347 -0.25338788 -0.36065738  0.57556251
  0.39429855  0.92418055  0.51310141  0.50863276 -0.34288184  0.25550695
  0.27922256 -0.63347346  0.51683045  0.58482732  1.01753168  0.49167673
 -0.93073806 -0.12910978  0.99982177 -0.26519293 -0.52838525  0.33717494
 -0.07051965  0.65693342 -1.06358983 -0.58510697 -0.5783339   0.41671184
 -0.73142046 -0.34312748  0.10572142  0.72962007 -0.68276441 -0.40820621
 -0.01353401 -0.37368866 -0.7934782  -0.77201205  0.19865261  0.39328153
  0.28499512  0.07401551 -0.76242358  0.34595454  0.75582845  0.88641801
 -0.65353359  1.01792651]

